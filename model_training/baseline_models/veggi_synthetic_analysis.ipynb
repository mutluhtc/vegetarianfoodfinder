{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNnql3XUvFBP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import classifier models \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic training dataset\n",
    "synthetic_train_df = pd.read_csv('train-test datasets/synthetic_train_v2.csv')\n",
    "\n",
    "# Yelp sample training dataset\n",
    "yelp_train_df = pd.read_csv('train-test datasets/yelp_sample_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dish_name        0\n",
       "description    243\n",
       "cuisine          0\n",
       "diet             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dish_name      0\n",
       "description    7\n",
       "cuisine        0\n",
       "diet           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Baseline Models\n",
    "\n",
    "We first test some standard machine learning classification models and evaluate their accuracy and F1-scores to see how good they are at labeling dishes as vegetarian or non-vegatarian.\n",
    "\n",
    "We will perform a 5-fold cross validation on the datasets `synthetic_train_df` and `yelp_train_df` separately, train the models on the combined training sets of the synthetic and yelp data in each fold, then have each model predict on the test sets of the synthetic data and yelp data separately (in each fold). This will tell us if the models are preforming differently on the synthetic vs the yelp sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_1 = {'Naive Bayes': MultinomialNB() ,\n",
    "                'Logistic Regression': LogisticRegression(solver='liblinear', class_weight='balanced'),\n",
    "                'XGBoost Classifier': XGBClassifier(n_estimators=100),\n",
    "                'SVM': SVC(),\n",
    "                'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "                'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, class_weight='balanced'),\n",
    "                'KNN': KNeighborsClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the text data\n",
    "X_synthetic_text = synthetic_train_df['dish_name'] + ': ' + synthetic_train_df['cuisine']  + ': ' + synthetic_train_df['description'].fillna('')\n",
    "X_synthetic_text = X_synthetic_text.apply(lambda x: re.sub(r'[^A-Za-z\\s]', '', x.lower())) # Clean text\n",
    "\n",
    "X_yelp_text = yelp_train_df['dish_name'] + ': ' + yelp_train_df['cuisine']  + ': ' + yelp_train_df['description'].fillna('') \n",
    "X_yelp_text = X_yelp_text.apply(lambda x: re.sub(r'[^A-Za-z\\s]', '', x.lower())) # Clean text\n",
    "\n",
    "# Target feature is 'diet' column \n",
    "y_synthetic_labels = synthetic_train_df['diet']\n",
    "y_yelp_labels = yelp_train_df['diet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF Vectorizer\n",
    "\n",
    "TF-IDF gives weight importance to words across documents. Frequent words are given lower scores, while unique words are given higher ones. Is appropiate for text classifications that depend on keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_calculator(mylist):\n",
    "    return 100*sum(mylist)/len(mylist)\n",
    "\n",
    "# Initiate the text encoders \n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df = .57, stop_words='english')\n",
    "le = LabelEncoder()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate accuracy and F1-score of each baseline classifier on the cross validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "for model_name, model in models_dict_1.items():\n",
    "\n",
    "    synthetic_acc_scores = []\n",
    "    synthetic_f1_scores = []\n",
    "    yelp_acc_scores = []\n",
    "    yelp_f1_scores = []\n",
    "\n",
    "    for fold, ((train_synth_idx, test_synth_idx), (train_yelp_idx, test_yelp_idx)) in enumerate(zip(kf.split(X_synthetic_text), kf.split(X_yelp_text)), 1):\n",
    "\n",
    "        # Create train-test splits for synthetic data\n",
    "        X_synthetic_train, X_synthetic_test = X_synthetic_text[train_synth_idx], X_synthetic_text[test_synth_idx]\n",
    "        y_synthetic_train, y_synthetic_test = y_synthetic_labels[train_synth_idx], y_synthetic_labels[test_synth_idx]\n",
    "\n",
    "        # Create train-test splits for yelp data\n",
    "        X_yelp_train, X_yelp_test = X_yelp_text[train_yelp_idx], X_yelp_text[test_yelp_idx]\n",
    "        y_yelp_train, y_yelp_test = y_yelp_labels[train_yelp_idx], y_yelp_labels[test_yelp_idx]\n",
    "\n",
    "        # Combine synthetic + yelp training data\n",
    "        X_combined_train_split = pd.concat([X_synthetic_train, X_yelp_train], axis=0)\n",
    "        y_combined_train_split = pd.concat([y_synthetic_train, y_yelp_train], axis=0)\n",
    "\n",
    "        # Encode the text data with TF-IDF \n",
    "        X_train_tfidf_split = tfidf_vectorizer.fit_transform(X_combined_train_split) # For fitting\n",
    "\n",
    "        X_test_tfidf_split_synthetic = tfidf_vectorizer.transform(X_synthetic_test)\n",
    "        X_test_tfidf_split_yelp = tfidf_vectorizer.transform(X_yelp_test)\n",
    "        \n",
    "        # Encode the target labels (Non-Veg --> 0, Veg --> 1)\n",
    "        y_train_encoded_split = le.fit_transform(y_combined_train_split) # For fitting\n",
    "\n",
    "        y_test_encoded_synthetic = le.transform(y_synthetic_test)\n",
    "        y_test_encoded_yelp = le.transform(y_yelp_test)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_tfidf_split, y_train_encoded_split)\n",
    "        \n",
    "        # Make predictions on synthetic data\n",
    "        y_pred_split_synthetic = model.predict(X_test_tfidf_split_synthetic)\n",
    "        synthetic_acc_scores.append(accuracy_score(y_pred_split_synthetic, y_test_encoded_synthetic))\n",
    "        synthetic_f1_scores.append(f1_score(y_pred_split_synthetic, y_test_encoded_synthetic))\n",
    "\n",
    "        # Make predictions on yelp data\n",
    "        y_pred_split_yelp = model.predict(X_test_tfidf_split_yelp)\n",
    "        yelp_acc_scores.append(accuracy_score(y_pred_split_yelp, y_test_encoded_yelp))\n",
    "        yelp_f1_scores.append(f1_score(y_pred_split_yelp, y_test_encoded_yelp))\n",
    "\n",
    "    score_dict[model_name] = {\"Avg. Accuracy Synthetic\": mean_calculator(synthetic_acc_scores),\n",
    "                              \"Avg. F1-Score Synthetic\": mean_calculator(synthetic_f1_scores),\n",
    "                              \"Avg. Accuracy Yelp\": mean_calculator(yelp_acc_scores),\n",
    "                              \"Avg. F1-Score Yelp\": mean_calculator(yelp_f1_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Accuracy Synthetic</th>\n",
       "      <th>Avg. F1-Score Synthetic</th>\n",
       "      <th>Avg. Accuracy Yelp</th>\n",
       "      <th>Avg. F1-Score Yelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost Classifier</th>\n",
       "      <td>94.882128</td>\n",
       "      <td>95.355453</td>\n",
       "      <td>89.446362</td>\n",
       "      <td>90.673702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>93.815887</td>\n",
       "      <td>94.272617</td>\n",
       "      <td>90.568703</td>\n",
       "      <td>91.452454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>93.442553</td>\n",
       "      <td>94.002839</td>\n",
       "      <td>86.863348</td>\n",
       "      <td>88.520668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>92.802553</td>\n",
       "      <td>93.283871</td>\n",
       "      <td>87.761597</td>\n",
       "      <td>89.117498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>89.923262</td>\n",
       "      <td>90.761110</td>\n",
       "      <td>84.733538</td>\n",
       "      <td>86.256471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>83.901135</td>\n",
       "      <td>84.933436</td>\n",
       "      <td>78.441404</td>\n",
       "      <td>82.553053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>82.035035</td>\n",
       "      <td>83.063341</td>\n",
       "      <td>76.879041</td>\n",
       "      <td>78.720813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Avg. Accuracy Synthetic  Avg. F1-Score Synthetic  \\\n",
       "XGBoost Classifier                 94.882128                95.355453   \n",
       "Decision Tree                      93.815887                94.272617   \n",
       "SVM                                93.442553                94.002839   \n",
       "Logistic Regression                92.802553                93.283871   \n",
       "Naive Bayes                        89.923262                90.761110   \n",
       "Random Forest                      83.901135                84.933436   \n",
       "KNN                                82.035035                83.063341   \n",
       "\n",
       "                     Avg. Accuracy Yelp  Avg. F1-Score Yelp  \n",
       "XGBoost Classifier            89.446362           90.673702  \n",
       "Decision Tree                 90.568703           91.452454  \n",
       "SVM                           86.863348           88.520668  \n",
       "Logistic Regression           87.761597           89.117498  \n",
       "Naive Bayes                   84.733538           86.256471  \n",
       "Random Forest                 78.441404           82.553053  \n",
       "KNN                           76.879041           78.720813  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(score_dict).T.sort_values(by=['Avg. F1-Score Synthetic','Avg. F1-Score Yelp'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cross Validation Fold Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change predictions back to string labels\n",
    "int_to_label = {1:'Vegetarian', 0:'Non-Vegetarian'}\n",
    "\n",
    "# Add fold dataframes to list\n",
    "combined_fold_prediction_dfs = []\n",
    "\n",
    "for model_name, model in models_dict_1.items():\n",
    "\n",
    "        for fold, ((train_synth_idx, test_synth_idx), (train_yelp_idx, test_yelp_idx)) in enumerate(zip(kf.split(X_synthetic_text), kf.split(X_yelp_text)), 1):\n",
    "\n",
    "                # print(f'==== Fold {fold} ====')\n",
    "\n",
    "                # Create train-test splits for synthetic data\n",
    "                X_synthetic_train, X_synthetic_test = X_synthetic_text[train_synth_idx], X_synthetic_text[test_synth_idx]\n",
    "                y_synthetic_train, y_synthetic_test = y_synthetic_labels[train_synth_idx], y_synthetic_labels[test_synth_idx]\n",
    "\n",
    "                # Create train-test splits for yelp data\n",
    "                X_yelp_train, X_yelp_test = X_yelp_text[train_yelp_idx], X_yelp_text[test_yelp_idx]\n",
    "                y_yelp_train, y_yelp_test = y_yelp_labels[train_yelp_idx], y_yelp_labels[test_yelp_idx]\n",
    "\n",
    "                # Combine synthetic + yelp training data\n",
    "                X_combined_train_split = pd.concat([X_synthetic_train, X_yelp_train], axis=0)\n",
    "                y_combined_train_split = pd.concat([y_synthetic_train, y_yelp_train], axis=0)\n",
    "\n",
    "                # Encode the text data with TF-IDF \n",
    "                X_train_tfidf_split = tfidf_vectorizer.fit_transform(X_combined_train_split) # For fitting\n",
    "                X_test_tfidf_split_synthetic = tfidf_vectorizer.transform(X_synthetic_test)\n",
    "                X_test_tfidf_split_yelp = tfidf_vectorizer.transform(X_yelp_test)\n",
    "                \n",
    "                # Encode the target labels (Non-Veg --> 0, Veg --> 1)\n",
    "                y_train_encoded_split = le.fit_transform(y_combined_train_split) # For fitting\n",
    "                y_test_encoded_synthetic = le.transform(y_synthetic_test)\n",
    "                y_test_encoded_yelp = le.transform(y_yelp_test)\n",
    "\n",
    "                # Train model\n",
    "                model.fit(X_train_tfidf_split, y_train_encoded_split)\n",
    "\n",
    "                # Predict (Synthetic)\n",
    "                y_pred_split_synthetic = model.predict(X_test_tfidf_split_synthetic)\n",
    "\n",
    "                synth_preds_df = pd.DataFrame({'original_index': test_synth_idx,\n",
    "                                        'predicted_diet': y_pred_split_synthetic}).set_index('original_index') # Ensure matching index columns\n",
    "                # Combine predictions \n",
    "                synth_fold_df = synthetic_train_df.loc[test_synth_idx].join(synth_preds_df['predicted_diet'])\n",
    "                # Convert predicted values to labels\n",
    "                synth_fold_df['predicted_diet'] = synth_fold_df['predicted_diet'].map(int_to_label)\n",
    "                # Add source column\n",
    "                synth_fold_df['source'] = 'Synthetic'\n",
    "\n",
    "                # Predict (Yelp)\n",
    "                y_pred_split_yelp = model.predict(X_test_tfidf_split_yelp)\n",
    "\n",
    "                yelp_preds_df = pd.DataFrame({'original_index': test_yelp_idx,\n",
    "                                        'predicted_diet': y_pred_split_yelp}).set_index('original_index')\n",
    "\n",
    "                yelp_fold_df = yelp_train_df.loc[test_yelp_idx].join(yelp_preds_df['predicted_diet'])\n",
    "                \n",
    "                yelp_fold_df['predicted_diet'] = yelp_fold_df['predicted_diet'].map(int_to_label)\n",
    "\n",
    "                yelp_fold_df['source'] = 'Yelp'\n",
    "\n",
    "                # Combine Synthetic & Yelp fold dfs\n",
    "                combined_fold_df = pd.concat([synth_fold_df, yelp_fold_df], axis=0).reset_index(drop=True)\n",
    "                combined_fold_prediction_dfs.append(combined_fold_df)\n",
    "\n",
    "                # Save each dataframe fold of model to a csv file\n",
    "                combined_fold_df.to_csv(f\"predictions_{model_name}_fold_{fold}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save each model's mismatch labels across all folds into a single csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_dict_1.keys():\n",
    "    fold_dfs = []\n",
    "    for fold in range(5):\n",
    "        fold_df = pd.read_csv(f'{model_name} Folds/predictions_{model_name}_fold_{fold+1}.csv')\n",
    "        fold_dfs.append(fold_df)\n",
    "\n",
    "    combined_folds_df = pd.concat(fold_dfs, axis=0)\n",
    "    combined_folds_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    unequal_rows_df = combined_folds_df.loc[combined_folds_df['diet'] != combined_folds_df['predicted_diet']].reset_index(drop=True)\n",
    "    unequal_rows_df.to_csv(f\"mislabeled_{model_name}.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "A **word embedding** is a NLP technique that represents a word as a numerical vector in a real vector space, which captures its sematic relationships where similar words have similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_2 = {'Naive Bayes': Pipeline([('scaler', MinMaxScaler()), ('clf', MultinomialNB())]),\n",
    "                'Logistic Regression': LogisticRegression(solver='liblinear', class_weight='balanced'),\n",
    "                'XGBoost Classifier': XGBClassifier(n_estimators=80),\n",
    "                'SVM': SVC(),\n",
    "                'Decision Tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "                'Random Forest': RandomForestClassifier(n_estimators=80, max_depth=5, class_weight='balanced'),\n",
    "                'KNN': KNeighborsClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/justinfong/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For word embedding\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab') # For NLTK version 3.8.2 or later, download 'punkt_tab' instead\n",
    "\n",
    "# Create document embeddings by averaging word vectors\n",
    "def get_document_embedding(sentence, word_vectors, vector_size):\n",
    "    embeddings = [word_vectors[word] for word in sentence if word in word_vectors]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate accuracy and F1-score of each baseline classifier on the cross validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "for model_name, model in models_dict_2.items():\n",
    "\n",
    "    synthetic_acc_scores = []\n",
    "    synthetic_f1_scores = []\n",
    "    yelp_acc_scores = []\n",
    "    yelp_f1_scores = []\n",
    "\n",
    "    for fold, ((train_synth_idx, test_synth_idx), (train_yelp_idx, test_yelp_idx)) in enumerate(zip(kf.split(X_synthetic_text), kf.split(X_yelp_text)), 1):\n",
    "\n",
    "        # Create train-test splits for synthetic data\n",
    "        X_synthetic_train, X_synthetic_test = X_synthetic_text[train_synth_idx], X_synthetic_text[test_synth_idx]\n",
    "        y_synthetic_train, y_synthetic_test = y_synthetic_labels[train_synth_idx], y_synthetic_labels[test_synth_idx]\n",
    "\n",
    "        # Create train-test splits for yelp data\n",
    "        X_yelp_train, X_yelp_test = X_yelp_text[train_yelp_idx], X_yelp_text[test_yelp_idx]\n",
    "        y_yelp_train, y_yelp_test = y_yelp_labels[train_yelp_idx], y_yelp_labels[test_yelp_idx]\n",
    "\n",
    "        # Combine synthetic + yelp training data\n",
    "        X_combined_train_split = pd.concat([X_synthetic_train, X_yelp_train], axis=0)\n",
    "        y_combined_train_split = pd.concat([y_synthetic_train, y_yelp_train], axis=0)\n",
    "\n",
    "        # Encode the text data with average word embeddings\n",
    "        token_combined_train = [word_tokenize(sentence.lower()) for sentence in X_combined_train_split] \n",
    "        word2vec_model = Word2Vec(sentences=token_combined_train, vector_size=100, window=5, min_count=1, workers=4)  \n",
    "        X_train_embeddings = np.array([get_document_embedding(s, word2vec_model.wv, word2vec_model.wv.vector_size) for s in token_combined_train])\n",
    "        \n",
    "        # Word embedd synthetic test data\n",
    "        token_synthetic = [word_tokenize(sentence.lower()) for sentence in X_synthetic_test]\n",
    "        word2vec_synthetic_model = Word2Vec(sentences=token_synthetic, vector_size=100, window=5, min_count=1, workers=4) \n",
    "        X_test_synthetic = np.array([get_document_embedding(s, word2vec_synthetic_model.wv, word2vec_synthetic_model.wv.vector_size) for s in token_synthetic])\n",
    "\n",
    "        # Word embedd yelp test data\n",
    "        token_yelp = [word_tokenize(sentence.lower()) for sentence in X_yelp_test]\n",
    "        word2vec_yelp_model = Word2Vec(sentences=token_yelp, vector_size=100, window=5, min_count=1, workers=4) \n",
    "        X_test_yelp = np.array([get_document_embedding(s, word2vec_yelp_model.wv, word2vec_yelp_model.wv.vector_size) for s in token_yelp])\n",
    "\n",
    "        \n",
    "        # Encode the target labels (Non-Veg --> 0, Veg --> 1)\n",
    "        y_train_encoded_split = le.fit_transform(y_combined_train_split) # For fitting\n",
    "\n",
    "        y_test_encoded_synthetic = le.transform(y_synthetic_test)\n",
    "        y_test_encoded_yelp = le.transform(y_yelp_test)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_embeddings, y_train_encoded_split)\n",
    "        \n",
    "        # Make predictions on synthetic data\n",
    "        y_pred_split_synthetic = model.predict(X_test_synthetic)\n",
    "        synthetic_acc_scores.append(accuracy_score(y_pred_split_synthetic, y_test_encoded_synthetic))\n",
    "        synthetic_f1_scores.append(f1_score(y_pred_split_synthetic, y_test_encoded_synthetic))\n",
    "\n",
    "        # Make predictions on yelp data\n",
    "        y_pred_split_yelp = model.predict(X_test_yelp)\n",
    "        yelp_acc_scores.append(accuracy_score(y_pred_split_yelp, y_test_encoded_yelp))\n",
    "        yelp_f1_scores.append(f1_score(y_pred_split_yelp, y_test_encoded_yelp))\n",
    "\n",
    "    score_dict[model_name] = {\"Avg. Accuracy Synthetic\": mean_calculator(synthetic_acc_scores),\n",
    "                              \"Avg. F1-Score Synthetic\": mean_calculator(synthetic_f1_scores),\n",
    "                              \"Avg. Accuracy Yelp\": mean_calculator(yelp_acc_scores),\n",
    "                              \"Avg. F1-Score Yelp\": mean_calculator(yelp_f1_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Accuracy Synthetic</th>\n",
       "      <th>Avg. F1-Score Synthetic</th>\n",
       "      <th>Avg. Accuracy Yelp</th>\n",
       "      <th>Avg. F1-Score Yelp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>54.316596</td>\n",
       "      <td>70.349471</td>\n",
       "      <td>53.423514</td>\n",
       "      <td>69.618941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>54.316596</td>\n",
       "      <td>70.349471</td>\n",
       "      <td>53.423514</td>\n",
       "      <td>69.618941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost Classifier</th>\n",
       "      <td>54.210355</td>\n",
       "      <td>70.197807</td>\n",
       "      <td>53.423514</td>\n",
       "      <td>69.618941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>53.465106</td>\n",
       "      <td>68.782169</td>\n",
       "      <td>53.423514</td>\n",
       "      <td>69.618941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>51.013759</td>\n",
       "      <td>63.931576</td>\n",
       "      <td>53.423514</td>\n",
       "      <td>69.618941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>53.197021</td>\n",
       "      <td>56.378203</td>\n",
       "      <td>50.277446</td>\n",
       "      <td>41.590436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>51.763262</td>\n",
       "      <td>43.661720</td>\n",
       "      <td>53.423514</td>\n",
       "      <td>69.618941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Avg. Accuracy Synthetic  Avg. F1-Score Synthetic  \\\n",
       "Naive Bayes                        54.316596                70.349471   \n",
       "SVM                                54.316596                70.349471   \n",
       "XGBoost Classifier                 54.210355                70.197807   \n",
       "Random Forest                      53.465106                68.782169   \n",
       "KNN                                51.013759                63.931576   \n",
       "Decision Tree                      53.197021                56.378203   \n",
       "Logistic Regression                51.763262                43.661720   \n",
       "\n",
       "                     Avg. Accuracy Yelp  Avg. F1-Score Yelp  \n",
       "Naive Bayes                   53.423514           69.618941  \n",
       "SVM                           53.423514           69.618941  \n",
       "XGBoost Classifier            53.423514           69.618941  \n",
       "Random Forest                 53.423514           69.618941  \n",
       "KNN                           53.423514           69.618941  \n",
       "Decision Tree                 50.277446           41.590436  \n",
       "Logistic Regression           53.423514           69.618941  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(score_dict).T.sort_values(by=['Avg. F1-Score Synthetic','Avg. F1-Score Yelp'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and F1-scores are lower using word embeddings. This may be due to training on a smaller dataset, and the fact that food descriptions are more keyword dependent than semantic meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on Combined Datasets\n",
    "\n",
    "We test how the baseline classifiers preform on the combined synthetic and yelp datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dish_name</th>\n",
       "      <th>description</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>diet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tom yum (hot &amp; sour) soup with prawns</td>\n",
       "      <td>700ml chicken stock 1 lemongrassstalk, bruised...</td>\n",
       "      <td>French</td>\n",
       "      <td>Non-Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Badshahi Chicken Recipe</td>\n",
       "      <td>Badshahi Chicken Recipe is a simple chicken re...</td>\n",
       "      <td>Indian</td>\n",
       "      <td>Non-Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plain Mixed Vegetable</td>\n",
       "      <td>with Steam Rice</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pork Lo Mein Tray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Non-Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vegetable Minimalist Tofu Spaghetti</td>\n",
       "      <td>A delicious and healthy vegan lunch option fea...</td>\n",
       "      <td>French</td>\n",
       "      <td>Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>Chicken With Mushroom Sauce Recipe</td>\n",
       "      <td>is a classic combination that leaves you droo...</td>\n",
       "      <td>Continental</td>\n",
       "      <td>Non-Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>Vegetable Spring Rolls</td>\n",
       "      <td>Crispy fried rolls filled with a medley of fin...</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>Instant Granola In A Microwave Recipe</td>\n",
       "      <td>Super crispy, crunchy, toasty granola, Instant...</td>\n",
       "      <td>Continental</td>\n",
       "      <td>Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>French Toast</td>\n",
       "      <td>Slices of bread soaked in an egg and milk mixt...</td>\n",
       "      <td>American</td>\n",
       "      <td>Vegetarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>Vegan Mushroom And Barley Soup</td>\n",
       "      <td>Barley is a whole grain rich in fiber and help...</td>\n",
       "      <td>Universal</td>\n",
       "      <td>Vegetarian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2767 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  dish_name  \\\n",
       "0     Tom yum (hot & sour) soup with prawns   \n",
       "1                   Badshahi Chicken Recipe   \n",
       "2                     Plain Mixed Vegetable   \n",
       "3                         Pork Lo Mein Tray   \n",
       "4       Vegetable Minimalist Tofu Spaghetti   \n",
       "...                                     ...   \n",
       "2762    Chicken With Mushroom Sauce Recipe    \n",
       "2763                 Vegetable Spring Rolls   \n",
       "2764  Instant Granola In A Microwave Recipe   \n",
       "2765                           French Toast   \n",
       "2766         Vegan Mushroom And Barley Soup   \n",
       "\n",
       "                                            description      cuisine  \\\n",
       "0     700ml chicken stock 1 lemongrassstalk, bruised...       French   \n",
       "1     Badshahi Chicken Recipe is a simple chicken re...       Indian   \n",
       "2                                       with Steam Rice      Chinese   \n",
       "3                                                   NaN      Chinese   \n",
       "4     A delicious and healthy vegan lunch option fea...       French   \n",
       "...                                                 ...          ...   \n",
       "2762   is a classic combination that leaves you droo...  Continental   \n",
       "2763  Crispy fried rolls filled with a medley of fin...      Chinese   \n",
       "2764  Super crispy, crunchy, toasty granola, Instant...  Continental   \n",
       "2765  Slices of bread soaked in an egg and milk mixt...     American   \n",
       "2766  Barley is a whole grain rich in fiber and help...    Universal   \n",
       "\n",
       "                diet  \n",
       "0     Non-Vegetarian  \n",
       "1     Non-Vegetarian  \n",
       "2         Vegetarian  \n",
       "3     Non-Vegetarian  \n",
       "4         Vegetarian  \n",
       "...              ...  \n",
       "2762  Non-Vegetarian  \n",
       "2763      Vegetarian  \n",
       "2764      Vegetarian  \n",
       "2765      Vegetarian  \n",
       "2766      Vegetarian  \n",
       "\n",
       "[2767 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([synthetic_train_df, yelp_train_df], axis=0).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "X = df['dish_name'] + ' ' + df['cuisine'] + ' ' + df['description']\n",
    "X = X.fillna('').apply(lambda x: re.sub(r'[^A-Za-z\\s]', '', x.lower())) # Clean text data for preparation\n",
    "y = df['diet']\n",
    "\n",
    "# Tokenize the sentences (rows of X)\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in X]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Calculate the average of the Word2Vec vectors of all words in a the tokenized document    \n",
    "document_embeddings = np.array([get_document_embedding(s, word2vec_model.wv, word2vec_model.wv.vector_size) for s in tokenized_corpus])\n",
    "\n",
    "# Encode the target labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_evaluations(model, cv=5):\n",
    "\n",
    "    scores = cross_val_score(model, document_embeddings, y_encoded, cv=5, scoring=\"f1\")\n",
    "    accuracy_scores = cross_val_score(model, document_embeddings, y_encoded, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "    d = {'average f1-score': scores.mean()*100,\n",
    "        'average accuracy': accuracy_scores.mean()*100}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average F1-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>72.543321</td>\n",
       "      <td>69.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>70.764440</td>\n",
       "      <td>59.775690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>70.376512</td>\n",
       "      <td>68.449155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>65.196646</td>\n",
       "      <td>64.763384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>64.383218</td>\n",
       "      <td>60.643030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>61.937932</td>\n",
       "      <td>60.426489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>52.229974</td>\n",
       "      <td>53.415045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Average F1-score  Average Accuracy\n",
       "0   XGBoost Classifier         72.543321         69.063200\n",
       "1                  SVM         70.764440         59.775690\n",
       "2  Logistic Regression         70.376512         68.449155\n",
       "3        Random Forest         65.196646         64.763384\n",
       "4        Decision Tree         64.383218         60.643030\n",
       "5                  KNN         61.937932         60.426489\n",
       "6          Naive Bayes         52.229974         53.415045"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate average accuracy and f1-scores on each cv-fold\n",
    "\n",
    "model_name_list = [model for model in models_dict_2.keys()]\n",
    "f1_score_list = [cross_val_evaluations(model)['average f1-score'] for model in models_dict_2.values()]\n",
    "accuracy_list = [cross_val_evaluations(model)['average accuracy'] for model in models_dict_2.values()]\n",
    "\n",
    "cv_score_dict = {'Model': model_name_list,\n",
    "              'Average F1-score': f1_score_list,\n",
    "              'Average Accuracy': accuracy_list}\n",
    "\n",
    "cv_score_df = pd.DataFrame(cv_score_dict).sort_values(by='Average F1-score', ascending=False).reset_index(drop=True)\n",
    "cv_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()\n",
    "y_copy = y.copy()\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X_copy)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_copy) # Non-Veg = 0, Veg = 1\n",
    "\n",
    "\n",
    "def cross_val_evaluations(model, cv=5):\n",
    "\n",
    "    scores = cross_val_score(model, X_tfidf, y_encoded, cv=5, scoring=\"f1\")\n",
    "    accuracy_scores = cross_val_score(model, X_tfidf, y_encoded, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "    d = {'average f1-score': scores.mean()*100,\n",
    "        'average accuracy': accuracy_scores.mean()*100}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Average F1-score</th>\n",
       "      <th>Average Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost Classifier</td>\n",
       "      <td>92.464988</td>\n",
       "      <td>91.434969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>92.227729</td>\n",
       "      <td>91.000973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>89.855971</td>\n",
       "      <td>88.434662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>89.267113</td>\n",
       "      <td>87.494533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>88.081064</td>\n",
       "      <td>86.447014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>83.042052</td>\n",
       "      <td>80.483089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>66.573319</td>\n",
       "      <td>54.824162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Average F1-score  Average Accuracy\n",
       "0   XGBoost Classifier         92.464988         91.434969\n",
       "1        Decision Tree         92.227729         91.000973\n",
       "2  Logistic Regression         89.855971         88.434662\n",
       "3                  SVM         89.267113         87.494533\n",
       "4          Naive Bayes         88.081064         86.447014\n",
       "5        Random Forest         83.042052         80.483089\n",
       "6                  KNN         66.573319         54.824162"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate average accuracy and f1-scores on each cv-fold\n",
    "\n",
    "model_name_list = [model for model in models_dict_1.keys()]\n",
    "f1_score_list = [cross_val_evaluations(model)['average f1-score'] for model in models_dict_1.values()]\n",
    "accuracy_list = [cross_val_evaluations(model)['average accuracy'] for model in models_dict_1.values()]\n",
    "\n",
    "cv_score_dict = {'Model': model_name_list,\n",
    "              'Average F1-score': f1_score_list,\n",
    "              'Average Accuracy': accuracy_list}\n",
    "\n",
    "cv_score_df = pd.DataFrame(cv_score_dict).sort_values(by='Average F1-score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "cv_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF still preforms better than word embedding, hence classifying vegetarian and non-vegetarian dishes may depend more on keywords such as chicken, pork, beef, ect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMT2RX3Hfwg4rRPNdw1gN7t",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
